{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLLLJX-dRfuh"
   },
   "source": [
    "# YouTube Channel View Prediction: A Machine Learning Project in Google Colab\n",
    "\n",
    "This Jupyter Notebook documents the process of building and evaluating machine learning models to predict YouTube channel view counts based on publicly available metadata. This version incorporates a **logarithmic transformation** on the target variable for improved model performance and interpretability, and is tailored for execution in Google Colaboratory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaXIVI9TRnDz"
   },
   "source": [
    "\n",
    "## 1. Setup and Library Imports\n",
    "\n",
    "This section imports all necessary Python libraries for data manipulation, machine learning, and evaluation. We also suppress warnings for cleaner output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr5sCACxRrEZ",
    "outputId": "53e5c0c9-e6ab-41c0-f5eb-efcb57143ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "import numpy as np # Import numpy for log transformation\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXOuDcR_RuMD"
   },
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Here, we'll load the \"Global YouTube Statistics\" dataset into a Pandas DataFrame. **Before running this cell, make sure you've uploaded your `Global-YouTube-Statistics.csv` file to your Colab environment or mounted your Google Drive.**\n",
    "\n",
    "If you chose to upload the file directly to Colab, it will reside in the current session's `/content/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooQEDoFaRx7e",
    "outputId": "2ed8f91c-b852-4127-9eec-f44ecd181649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Dataset loaded successfully.\n",
      "Initial shape: (995, 28)\n",
      "Columns: ['rank', 'Youtuber', 'subscribers', 'video views', 'category', 'Title', 'uploads', 'Country', 'Abbreviation', 'channel_type', 'video_views_rank', 'country_rank', 'channel_type_rank', 'video_views_for_the_last_30_days', 'lowest_monthly_earnings', 'highest_monthly_earnings', 'lowest_yearly_earnings', 'highest_yearly_earnings', 'subscribers_for_last_30_days', 'created_year', 'created_month', 'created_date', 'Gross tertiary education enrollment (%)', 'Population', 'Unemployment rate', 'Urban_population', 'Latitude', 'Longitude']\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   rank                    Youtuber  subscribers   video views  \\\n",
      "0     1                    T-Series    245000000  2.280000e+11   \n",
      "1     2              YouTube Movies    170000000  0.000000e+00   \n",
      "2     3                     MrBeast    166000000  2.836884e+10   \n",
      "3     4  Cocomelon - Nursery Rhymes    162000000  1.640000e+11   \n",
      "4     5                   SET India    159000000  1.480000e+11   \n",
      "\n",
      "           category                       Title  uploads        Country  \\\n",
      "0             Music                    T-Series    20082          India   \n",
      "1  Film & Animation               youtubemovies        1  United States   \n",
      "2     Entertainment                     MrBeast      741  United States   \n",
      "3         Education  Cocomelon - Nursery Rhymes      966  United States   \n",
      "4             Shows                   SET India   116536          India   \n",
      "\n",
      "  Abbreviation   channel_type  ...  subscribers_for_last_30_days  \\\n",
      "0           IN          Music  ...                     2000000.0   \n",
      "1           US          Games  ...                           NaN   \n",
      "2           US  Entertainment  ...                     8000000.0   \n",
      "3           US      Education  ...                     1000000.0   \n",
      "4           IN  Entertainment  ...                     1000000.0   \n",
      "\n",
      "   created_year  created_month  created_date  \\\n",
      "0        2006.0            Mar          13.0   \n",
      "1        2006.0            Mar           5.0   \n",
      "2        2012.0            Feb          20.0   \n",
      "3        2006.0            Sep           1.0   \n",
      "4        2006.0            Sep          20.0   \n",
      "\n",
      "   Gross tertiary education enrollment (%)    Population  Unemployment rate  \\\n",
      "0                                     28.1  1.366418e+09               5.36   \n",
      "1                                     88.2  3.282395e+08              14.70   \n",
      "2                                     88.2  3.282395e+08              14.70   \n",
      "3                                     88.2  3.282395e+08              14.70   \n",
      "4                                     28.1  1.366418e+09               5.36   \n",
      "\n",
      "   Urban_population   Latitude  Longitude  \n",
      "0       471031528.0  20.593684  78.962880  \n",
      "1       270663028.0  37.090240 -95.712891  \n",
      "2       270663028.0  37.090240 -95.712891  \n",
      "3       270663028.0  37.090240 -95.712891  \n",
      "4       471031528.0  20.593684  78.962880  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "print(\"Loading Data...\")\n",
    "try:\n",
    "    # Option 1: If you uploaded the file directly to Colab (e.g., using files.upload())\n",
    "    # The file will be in the default content directory.\n",
    "    # Try different encodings if 'utf-8' fails. Common alternatives are 'latin1' or 'cp1252'.\n",
    "    df = pd.read_csv('./Global-YouTube-Statistics.csv', encoding='latin1')\n",
    "\n",
    "    # If 'latin1' doesn't work, try 'cp1252':\n",
    "    # df = pd.read_csv('Global-YouTube-Statistics.csv', encoding='cp1252')\n",
    "\n",
    "    # Option 2: If you mounted Google Drive (uncomment and adjust path if needed)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # df = pd.read_csv('/content/drive/MyDrive/path/to/Global-YouTube-Statistics.csv', encoding='latin1') # <--- ADJUST THIS PATH AND ADD ENCODING\n",
    "\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Global-YouTube-Statistics.csv' not found. Please ensure the file is in the correct directory or uploaded.\")\n",
    "    print(\"If you just uploaded it, restart the kernel and try again, or check the file name carefully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    print(\"Consider trying a different 'encoding' parameter in pd.read_csv() if the error persists (e.g., encoding='cp1252').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQo-snqPR0tD"
   },
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "This is a crucial step where we clean the data, handle missing values, and transform features into a format suitable for machine learning models.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "We'll fill missing numerical values with the column's median and missing categorical values with the mode. This prevents errors during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtPjM8aER4Rn",
    "outputId": "51b1db69-2dd7-4f12-aee8-59c4c07e9cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "  Filled missing values in 'video_views_rank' with median: 915.5\n",
      "  Filled missing values in 'country_rank' with median: 51.0\n",
      "  Filled missing values in 'channel_type_rank' with median: 65.5\n",
      "  Filled missing values in 'video_views_for_the_last_30_days' with median: 64085000.0\n",
      "  Filled missing values in 'subscribers_for_last_30_days' with median: 200000.0\n",
      "  Filled missing values in 'created_year' with median: 2013.0\n",
      "  Filled missing values in 'created_date' with median: 16.0\n",
      "  Filled missing values in 'Gross tertiary education enrollment (%)' with median: 68.0\n",
      "  Filled missing values in 'Population' with median: 328239523.0\n",
      "  Filled missing values in 'Unemployment rate' with median: 9.365\n",
      "  Filled missing values in 'Urban_population' with median: 270663028.0\n",
      "  Filled missing values in 'Latitude' with median: 37.09024\n",
      "  Filled missing values in 'Longitude' with median: -51.92528\n",
      "  Filled missing values in 'category' with mode: Entertainment\n",
      "  Filled missing values in 'Country' with mode: United States\n",
      "  Filled missing values in 'Abbreviation' with mode: US\n",
      "  Filled missing values in 'channel_type' with mode: Entertainment\n",
      "  Filled missing values in 'created_month' with mode: Jan\n",
      "Shape after handling NaNs/Infs: (995, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Handling missing values...\")\n",
    "# For numerical columns, fill with median\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"  Filled missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "# For categorical columns, fill with mode\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().any():\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(mode_val)\n",
    "        print(f\"  Filled missing values in '{col}' with mode: {mode_val}\")\n",
    "\n",
    "# Ensure no remaining NaN/infinite values before proceeding\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Shape after handling NaNs/Infs: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f2Gga0cR7qV"
   },
   "source": [
    "### Feature Creation: `created_date_numeric`\n",
    "\n",
    "We combine `created_year` and `created_month` into a single numerical feature to represent the channel's creation timeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLU_fkD0R-wx",
    "outputId": "650ccb76-9248-4d4f-a50d-3eed124c2597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'created_date_numeric' feature.\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Combine created_year and created_month into a single numerical feature\n",
    "# Convert month abbreviations to numerical month (1-12)\n",
    "df['created_month_numeric'] = df['created_month'].apply(lambda x: pd.to_datetime(x, format='%b').month)\n",
    "df['created_date_numeric'] = df['created_year'] + (df['created_month_numeric'] / 12)\n",
    "print(\"Created 'created_date_numeric' feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS1H979WSBmh"
   },
   "source": [
    "### Categorical Feature Encoding\n",
    "\n",
    "Categorical features like `category` and `Country` need to be converted into numerical representations. We'll use `LabelEncoder` for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bimo950MSEOz",
    "outputId": "60d04000-5d1e-43e2-bb17-8aa5f54d1a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features...\n",
      "  Encoded 'category' into 'category_encoded'.\n",
      "  Encoded 'Country' into 'Country_encoded'.\n",
      "  Encoded 'channel_type' into 'channel_type_encoded'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding categorical features...\")\n",
    "label_encoders = {}\n",
    "for col in ['category', 'Country', 'channel_type']: # 'created_month' is now used in 'created_date_numeric'\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  Encoded '{col}' into '{col}_encoded'.\")\n",
    "    else:\n",
    "        print(f\"  Warning: Column '{col}' not found for encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC7PryUeSGIq"
   },
   "source": [
    "### Feature and Target Selection\n",
    "\n",
    "We define the set of input features (X) and apply a `logarithmic transformation` to the `video views` target variable (y). This helps to handle the skewed distribution often found in view counts, which can significantly improve model performance and make evaluation metrics more meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMEVbNzISNxm",
    "outputId": "94952ff8-a66d-4190-9abe-479577c7a25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log1p transformation to the target variable 'video views'.\n",
      "Remember to use np.expm1() on predictions to convert them back to the original scale.\n",
      "Selected 9 features and target 'video views'.\n",
      "X shape: (995, 9), y shape: (995,)\n"
     ]
    }
   ],
   "source": [
    "# Select features and target variable\n",
    "# MODIFIED: Using a selected subset of features (e.g., top 10 from previous analysis)\n",
    "features = [\n",
    "    'subscribers',\n",
    "    'video_views_rank',\n",
    "    'created_date_numeric',\n",
    "    'channel_type_rank',\n",
    "    'uploads',\n",
    "    'video_views_for_the_last_30_days',\n",
    "    'subscribers_for_last_30_days',\n",
    "    'category_encoded',\n",
    "    'lowest_yearly_earnings'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all selected features exist in the DataFrame\n",
    "existing_features = [f for f in features if f in df.columns]\n",
    "if len(existing_features) != len(features):\n",
    "    missing = set(features) - set(existing_features)\n",
    "    print(f\"Warning: The following features were not found in the dataset and will be excluded: {missing}\")\n",
    "    features = existing_features\n",
    "\n",
    "target = 'video views' # Your chosen target\n",
    "\n",
    "X = df[features]\n",
    "# Apply logarithmic transformation (log1p to handle zero values gracefully)\n",
    "y = np.log1p(df[target])\n",
    "print(f\"Applied log1p transformation to the target variable '{target}'.\")\n",
    "print(\"Remember to use np.expm1() on predictions to convert them back to the original scale.\")\n",
    "\n",
    "print(f\"Selected {len(features)} features and target '{target}'.\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0y17G57SQbx"
   },
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Numerical features are scaled using `StandardScaler` to normalize their range. This is particularly important for SVR and can improve the performance of tree-based models as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIKr9WyASRMd",
    "outputId": "53f69ac2-1a34-4346-bf73-ec6d647d0f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling numerical features...\n",
      "Numerical features scaled.\n",
      "\n",
      "First 5 rows of the scaled feature set (X):\n",
      "   subscribers  video_views_rank  created_date_numeric  channel_type_rank  \\\n",
      "0    12.674193         -0.406670             -1.536414          -0.377155   \n",
      "1     8.392710          2.571724             -1.536414           3.499058   \n",
      "2     8.164364         -0.406636             -0.222450          -0.377155   \n",
      "3     7.936019         -0.406670             -1.425374          -0.377155   \n",
      "4     7.764759         -0.406669             -1.425374          -0.376633   \n",
      "\n",
      "    uploads  video_views_for_the_last_30_days  subscribers_for_last_30_days  \\\n",
      "0  0.319178                          5.156029                      3.374616   \n",
      "1 -0.269118                         -0.418012                     -0.195539   \n",
      "2 -0.247439                          2.909626                     15.275134   \n",
      "3 -0.240847                          4.457422                      1.391196   \n",
      "4  3.144908                          4.084667                      1.391196   \n",
      "\n",
      "   category_encoded  lowest_yearly_earnings  \n",
      "0          0.573658                7.385997  \n",
      "1         -0.542801               -0.513785  \n",
      "2         -0.821916                4.133146  \n",
      "3         -1.101030                6.340438  \n",
      "4          2.248346                5.875745  \n"
     ]
    }
   ],
   "source": [
    "print(\"Scaling numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "numerical_features_to_scale = X.select_dtypes(include=np.number).columns.tolist()\n",
    "X[numerical_features_to_scale] = scaler.fit_transform(X[numerical_features_to_scale])\n",
    "print(\"Numerical features scaled.\")\n",
    "print(\"\\nFirst 5 rows of the scaled feature set (X):\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYlu-fa2SeW_"
   },
   "source": [
    "## 4. Data Splitting (No Longer for Primary Evaluation)\n",
    "\n",
    "While we traditionally split data into training and testing, for k-fold cross-validation, we will use the entire dataset for cross-validation. The `train_test_split` is not directly needed for the primary evaluation with `cross_val_score`, as `cross_val_score` handles the internal splitting for evaluation. However, if you intend to train a final model on the full dataset after evaluating with cross-validation, or to perform hyperparameter tuning with a separate validation set, `train_test_split` could still be used. For now, we'll proceed directly to cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kibDD9uSgVh",
    "outputId": "5973aee2-8dc1-4bf4-a770-ec4811fe9508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to K-Fold Cross-Validation and Hyperparameter Tuning for robust model evaluation.\n"
     ]
    }
   ],
   "source": [
    "# The following lines are commented out as cross_val_score uses the full dataset for internal splits.\n",
    "# If you plan to use a dedicated test set AFTER cross-validation for a final, unbiased evaluation\n",
    "# of the *best* model (e.g., after hyperparameter tuning), you would uncomment and use this split.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(f\"Training set shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "# print(f\"Testing set shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "print(\"Proceeding to K-Fold Cross-Validation and Hyperparameter Tuning for robust model evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm0wlr0OShqp"
   },
   "source": [
    "## 5. Model Training and Evaluation with K-Fold Cross-Validation\n",
    "\n",
    "This section performs k-fold cross-validation for each of your chosen models (XGBoost, Random Forest, SVR). This provides a more reliable estimate of model performance than a single train/test split. We'll then proceed to fine-tune XGBoost and Random Forest using Grid Search and Randomized Search.\n",
    "\n",
    "We'll use R-squared as the primary scoring metric, and also display Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx5txBbvSjap",
    "outputId": "2ca2d9cc-17a1-4dcd-dfa9-07051dc6f382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1 Initial K-Fold Cross-Validation (Baseline) ...\n",
      "Performing 5-fold cross-validation for each model...\n",
      "\n",
      "  Evaluating XGBoost model...\n",
      "    XGBoost - Mean R-squared (R2): 0.4052 (Std: 0.2044)\n",
      "    XGBoost - Mean Absolute Error (MAE) (log-scale): 0.3696 (Std: 0.2202)\n",
      "  Evaluating Random Forest model...\n",
      "    Random Forest - Mean R-squared (R2): 0.0224 (Std: 0.4569)\n",
      "    Random Forest - Mean Absolute Error (MAE) (log-scale): 0.4249 (Std: 0.2094)\n",
      "  Evaluating SVR model...\n",
      "    SVR - Mean R-squared (R2): 0.0390 (Std: 0.0709)\n",
      "    SVR - Mean Absolute Error (MAE) (log-scale): 0.7111 (Std: 0.2965)\n",
      "\n",
      "--- Summary of Initial K-Fold Cross-Validation Performance ---\n",
      "Model: XGBoost\n",
      "  Mean R2: 0.4052 (Std: 0.2044)\n",
      "  Mean MAE (log-scale): 0.3696 (Std: 0.2202)\n",
      "------------------------------\n",
      "Model: Random Forest\n",
      "  Mean R2: 0.0224 (Std: 0.4569)\n",
      "  Mean MAE (log-scale): 0.4249 (Std: 0.2094)\n",
      "------------------------------\n",
      "Model: SVR\n",
      "  Mean R2: 0.0390 (Std: 0.0709)\n",
      "  Mean MAE (log-scale): 0.7111 (Std: 0.2965)\n",
      "------------------------------\n",
      "\n",
      "Initial K-Fold Cross-Validation finished. Proceeding to Hyperparameter Tuning.\n"
     ]
    }
   ],
   "source": [
    "# --- 5.1 Initial K-Fold Cross-Validation (Baseline) ---\n",
    "print(\"5.1 Initial K-Fold Cross-Validation (Baseline) ...\")\n",
    "\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "n_splits = 5 # Number of folds for cross-validation\n",
    "\n",
    "# Define a scorer for Mean Absolute Error (MAE)\n",
    "# Note: For log-transformed target, MAE will be on the log scale.\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation for each model...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Evaluating {name} model...\")\n",
    "    try:\n",
    "        # Use R-squared as the primary metric for cross_val_score\n",
    "        # Changed n_jobs from -1 to 1 to avoid _posixsubprocess error\n",
    "        r2_scores = cross_val_score(model, X, y, cv=n_splits, scoring='r2', n_jobs=1)\n",
    "        mean_r2 = r2_scores.mean()\n",
    "        std_r2 = r2_scores.std()\n",
    "\n",
    "        # To get MAE, use neg_mean_absolute_error and convert back to positive\n",
    "        # MAE here will be on the log-transformed scale\n",
    "        # Changed n_jobs from -1 to 1 to 1 to avoid _posixsubprocess error\n",
    "        neg_mae_scores = cross_val_score(model, X, y, cv=n_splits, scoring='neg_mean_absolute_error', n_jobs=1)\n",
    "        mean_mae = -neg_mae_scores.mean()\n",
    "        std_mae = neg_mae_scores.std()\n",
    "\n",
    "        cv_results[name] = {\n",
    "            'Mean R2': mean_r2,\n",
    "            'Std R2': std_r2,\n",
    "            'Mean MAE': mean_mae,\n",
    "            'Std MAE': std_mae\n",
    "        }\n",
    "        print(f\"    {name} - Mean R-squared (R2): {mean_r2:.4f} (Std: {std_r2:.4f})\")\n",
    "        print(f\"    {name} - Mean Absolute Error (MAE) (log-scale): {mean_mae:.4f} (Std: {std_mae:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error evaluating {name} with cross-validation: {e}\")\n",
    "        cv_results[name] = {'Mean R2': 'Error', 'Std R2': 'Error', 'Mean MAE': 'Error', 'Std MAE': 'Error'}\n",
    "\n",
    "print(\"\\n--- Summary of Initial K-Fold Cross-Validation Performance ---\")\n",
    "for name, metrics in cv_results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    # Conditional formatting: check if the value is a float or int before formatting\n",
    "    print(f\"  Mean R2: {metrics['Mean R2']:.4f}\" if isinstance(metrics['Mean R2'], (float, int)) else f\"  Mean R2: {metrics['Mean R2']}\",\n",
    "          f\"(Std: {metrics['Std R2']:.4f})\" if isinstance(metrics['Std R2'], (float, int)) else f\"(Std: {metrics['Std R2']})\")\n",
    "    print(f\"  Mean MAE (log-scale): {metrics['Mean MAE']:.4f}\" if isinstance(metrics['Mean MAE'], (float, int)) else f\"  Mean MAE (log-scale): {metrics['Mean MAE']}\",\n",
    "          f\"(Std: {metrics['Std MAE']:.4f})\" if isinstance(metrics['Std MAE'], (float, int)) else f\"(Std: {metrics['Std MAE']})\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\nInitial K-Fold Cross-Validation finished. Proceeding to Hyperparameter Tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAm0PtdSa94S"
   },
   "source": [
    "### 5.2 Hyperparameter Tuning with GridSearchCV (for XGBoost)\n",
    "We'll use `GridSearchCV` to systematically search for the best combination of hyperparameters for the XGBoost model. This can be computationally intensive if the grid is too large. We aim for the highest R-squared score.\n",
    "\n",
    "Note: For a truly exhaustive search and potentially higher scores, you might need to broaden these ranges and increase `n_estimators` (e.g., up to 500 or 1000) and/or `max_depth` (e.g., up to 10 or 15), but this will significantly increase computation time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXxcX4EpbC0H",
    "outputId": "ce86f838-ecc3-46a8-a7d5-1e3cbabc6131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.2 Hyperparameter Tuning with GridSearchCV (for XGBoost)...\n",
      "Starting GridSearchCV for XGBoost. This may take some time...\n",
      "\n",
      "--- GridSearchCV Results for XGBoost ---\n",
      "Best parameters found: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Best R-squared score (from cross-validation): 0.4375\n"
     ]
    }
   ],
   "source": [
    "# --- 5.2 Hyperparameter Tuning with GridSearchCV (for XGBoost) ---\n",
    "print(\"\\n5.2 Hyperparameter Tuning with GridSearchCV (for XGBoost)...\")\n",
    "\n",
    "# Define the parameter grid for XGBoost - EXPANDED RANGES\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 300, 500], # Increased max estimators\n",
    "    'max_depth': [3, 5, 7],         # Increased max depth\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1], # Finer granularity and lower values\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='r2',  # Optimize for R-squared\n",
    "    cv=n_splits,   # Use the same number of folds as before\n",
    "    n_jobs=1,     # Changed from -1 to 1 to avoid _posixsubprocess error\n",
    "    verbose=0      # Changed from 2 to 0 to reduce print output\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for XGBoost. This may take some time...\")\n",
    "grid_search_xgb.fit(X, y) # Fit on the full data for cross-validation\n",
    "\n",
    "print(\"\\n--- GridSearchCV Results for XGBoost ---\")\n",
    "print(f\"Best parameters found: {grid_search_xgb.best_params_}\")\n",
    "print(f\"Best R-squared score (from cross-validation): {grid_search_xgb.best_score_:.4f}\")\n",
    "\n",
    "# You can also access the full results:\n",
    "# cv_results_xgb = pd.DataFrame(grid_search_xgb.cv_results_)\n",
    "# print(\"\\nFull GridSearchCV results (first 5 rows):\")\n",
    "# print(cv_results_xgb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFGyFQY6bGx0"
   },
   "source": [
    "### 5.3 Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)\n",
    "\n",
    "`RandomizedSearchCV` is generally more efficient for larger parameter spaces as it samples a fixed number of parameter settings from specified distributions. This is often preferred over `GridSearchCV` when the search space is vast.\n",
    "\n",
    "Note: We've increased `n_iter` to sample more combinations from the expanded parameter distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_uOxanYbNCs",
    "outputId": "c6edeac9-2732-45e3-d770-67a192a9e2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3 Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)...\n",
      "Starting RandomizedSearchCV for Random Forest. This may take some time...\n",
      "\n",
      "--- RandomizedSearchCV Results for Random Forest ---\n",
      "Best parameters found: {'bootstrap': True, 'max_depth': 14, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 576}\n",
      "Best R-squared score (from cross-validation): 0.3186\n"
     ]
    }
   ],
   "source": [
    "# --- 5.3 Hyperparameter Tuning with RandomizedSearchCV (for Random Forest) ---\n",
    "print(\"\\n5.3 Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)...\")\n",
    "\n",
    "from scipy.stats import randint, uniform # For defining distributions\n",
    "\n",
    "# Define the parameter distributions for Random Forest - EXPANDED RANGES\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(200, 800), # Increased range for estimators\n",
    "    'max_depth': randint(8, 25),     # Increased range for max depth\n",
    "    'min_samples_split': randint(2, 25), # Slightly wider range\n",
    "    'min_samples_leaf': randint(1, 15),  # Slightly wider range\n",
    "    'max_features': ['auto', 'sqrt', 'log2', uniform(0.5, 0.4)], # Explore a continuous range for fractions\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=100,      # Increased number of sampled settings (from 50 to 100)\n",
    "    scoring='r2',    # Optimize for R-squared\n",
    "    cv=n_splits,     # Use the same number of folds\n",
    "    random_state=42, # For reproducibility\n",
    "    n_jobs=1,       # Changed from -1 to 1 to avoid _posixsubprocess error\n",
    "    verbose=0        # Changed from 2 to 0 to reduce print output\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearchCV for Random Forest. This may take some time...\")\n",
    "random_search_rf.fit(X, y) # Fit on the full data for cross-validation\n",
    "\n",
    "print(\"\\n--- RandomizedSearchCV Results for Random Forest ---\")\n",
    "print(f\"Best parameters found: {random_search_rf.best_params_}\")\n",
    "print(f\"Best R-squared score (from cross-validation): {random_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# You can also access the full results:\n",
    "# cv_results_rf = pd.DataFrame(random_search_rf.cv_results_)\n",
    "# print(\"\\nFull RandomizedSearchCV results (first 5 rows):\")\n",
    "# print(cv_results_rf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lgo1mbLQa65E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StsKTvMWSl5x"
   },
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "This notebook now provides a comprehensive machine learning pipeline, including initial baseline evaluation with k-fold cross-validation and subsequent hyperparameter tuning using both GridSearchCV and RandomizedSearchCV for XGBoost and Random Forest. The \"Best R-squared score\" from these searches represents the most robust performance you've achieved through systematic tuning.\n",
    "\n",
    "To further improve your model's performance and prepare for your project report, consider these next steps:\n",
    "\n",
    "1. Re-run the Notebook: Execute all cells in the notebook from top to bottom. The expanded search spaces for hyperparameters in sections 5.2 and 5.3 may find better-performing model configurations. This might take longer to run due to the wider search.\n",
    "\n",
    "2. Analyze New Tuning Results: Pay close attention to the \"Best R-squared score\" from the tuning processes. Hopefully, these will be higher than the initial baseline R2s.\n",
    "\n",
    "3. Advanced Feature Engineering: This is often the most impactful way to significantly boost model performance, especially when R-squared values are still moderate. Consider creating:\n",
    "\n",
    "  - Ratio features: For example, subscribers_to_views_ratio (subscribers / video views), or uploads_per_year (uploads / channel_age).\n",
    "\n",
    "  - Interaction features: Products of two features, if you suspect they have a combined effect (e.g., uploads * subscribers).\n",
    "\n",
    "  - Polynomial features: If you suspect non-linear relationships that your current models aren't fully capturing. (Be cautious with overfitting here).\n",
    "\n",
    "  - Aggregation features: If you have any grouped data (e.g., average views per category).\n",
    "\n",
    "4. Final Model Training: Once you have the best_params_ from GridSearchCV (for XGBoost) and RandomizedSearchCV (for Random Forest) after the re-run, you should train your final, optimized model using these optimal parameters on the entire dataset (X and y). This will be the model you'd use for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckM__JaUbcbO",
    "outputId": "6d46cac3-9aea-4a62-a2e1-ad03eb6d76df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6.1 Final Model Training and Evaluation on Original Scale...\n",
      "\n",
      "Training final RandomForestRegressor model with best parameters on the full dataset...\n",
      "Final model trained.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/Global-YouTube-Statistics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal model trained.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Now, let's evaluate this final model on a new test split to get original scale metrics\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# We need the original 'video views' (target) for comparison.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Make sure to get 'y_original_scale' from the original DataFrame before log-transforming.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df_original_y = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/content/data/Global-YouTube-Statistics.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Re-load or store original y\u001b[39;00m\n\u001b[32m     17\u001b[39m y_original_scale = df_original_y[target] \u001b[38;5;66;03m# Get the original, untransformed target\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Split data for final evaluation. Use the same random_state for reproducibility.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# X is already scaled and encoded, y is log-transformed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/data/Global-YouTube-Statistics.csv'"
     ]
    }
   ],
   "source": [
    "# --- 6.1 Final Model Training and Evaluation on Original Scale ---\n",
    "print(\"\\n6.1 Final Model Training and Evaluation on Original Scale...\")\n",
    "\n",
    "# For demonstration, let's pick the Random Forest model as it had a slightly better R2.\n",
    "# You can choose XGBoost instead if its tuned performance becomes better.\n",
    "best_model_params = random_search_rf.best_params_\n",
    "final_model = RandomForestRegressor(**best_model_params, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining final {type(final_model).__name__} model with best parameters on the full dataset...\")\n",
    "final_model.fit(X, y) # Train on the entire log-transformed dataset\n",
    "print(\"Final model trained.\")\n",
    "\n",
    "# Now, let's evaluate this final model on a new test split to get original scale metrics\n",
    "# We need the original 'video views' (target) for comparison.\n",
    "# Make sure to get 'y_original_scale' from the original DataFrame before log-transforming.\n",
    "df_original_y = pd.read_csv('./Global-YouTube-Statistics.csv', encoding='latin1') # Re-load or store original y\n",
    "y_original_scale = df_original_y[target] # Get the original, untransformed target\n",
    "\n",
    "# Split data for final evaluation. Use the same random_state for reproducibility.\n",
    "# X is already scaled and encoded, y is log-transformed.\n",
    "X_train_final, X_test_final, y_train_log, y_test_log = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the corresponding original y_test values\n",
    "# Make sure the indices align after splitting.\n",
    "y_test_original_scale = y_original_scale.loc[y_test_log.index]\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating final model on a held-out test set (original scale)...\")\n",
    "# Make predictions on the log-transformed test set\n",
    "y_pred_log = final_model.predict(X_test_final)\n",
    "\n",
    "# Convert predictions AND actual values back to original scale for MAE/R2 calculation\n",
    "y_pred_original_scale = np.expm1(y_pred_log)\n",
    "y_test_original_for_eval = np.expm1(y_test_log) # This is crucial: the true values were also log-transformed\n",
    "\n",
    "# Calculate metrics on the original scale\n",
    "mae_original = mean_absolute_error(y_test_original_for_eval, y_pred_original_scale)\n",
    "r2_original = r2_score(y_test_original_for_eval, y_pred_original_scale)\n",
    "\n",
    "print(f\"  Final Model (Random Forest) - MAE (Original Scale): {mae_original:.2f}\")\n",
    "print(f\"  Final Model (Random Forest) - R-squared (Original Scale): {r2_original:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6.2 Example: Making a Prediction for a Hypothetical New Channel ---\n",
    "print(\"\\n6.2 Example: Predicting for a Hypothetical New Channel...\")\n",
    "\n",
    "# Create a sample new channel's features (ensure it matches your feature list)\n",
    "# Example data (hypothetical values, remember to scale categorical and numerical features appropriately)\n",
    "# This is a raw example, in a real scenario, you'd apply the same preprocessing steps (encoding, scaling)\n",
    "# as you did for your training data.\n",
    "\n",
    "# Create a DataFrame for a single new sample. This will require manual encoding/scaling.\n",
    "# For simplicity, let's take the first row of your original (scaled) X as an example,\n",
    "# and pretend it's a 'new' channel to show the full prediction process.\n",
    "# In a real scenario, you would have raw data for a new channel, then apply your label_encoders and scaler.\n",
    "sample_new_channel_features_scaled = X.iloc[[0]] # Get one row from your already processed X\n",
    "sample_original_views = np.expm1(y.iloc[0]) # Get its original views for comparison\n",
    "\n",
    "print(f\"Hypothetical channel's actual views (from dataset, original scale): {sample_original_views:.0f}\")\n",
    "\n",
    "# Predict on the log scale\n",
    "predicted_log_views = final_model.predict(sample_new_channel_features_scaled)\n",
    "\n",
    "# Convert prediction back to original scale\n",
    "predicted_original_views = np.expm1(predicted_log_views)\n",
    "\n",
    "print(f\"Predicted views for hypothetical channel (log-scale): {predicted_log_views[0]:.4f}\")\n",
    "print(f\"Predicted views for hypothetical channel (original scale): {predicted_original_views[0]:.0f}\")\n",
    "\n",
    "# --- 6.3 Feature Importance Analysis ---\n",
    "print(\"\\n6.3 Feature Importance Analysis...\")\n",
    "\n",
    "# Feature importances from the best-tuned XGBoost model\n",
    "xgb_feature_importances = final_xgb_model.feature_importances_\n",
    "xgb_features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- XGBoost Feature Importances ---\")\n",
    "print(xgb_features_df)\n",
    "\n",
    "# Feature importances from the best-tuned Random Forest model\n",
    "rf_feature_importances = final_rf_model.feature_importances_\n",
    "rf_features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Random Forest Feature Importances ---\")\n",
    "print(rf_features_df)\n",
    "\n",
    "# Optional: You can also average the importances if you want a combined view\n",
    "# Ensure features are in the same order\n",
    "# averaged_importances = (xgb_feature_importances + rf_feature_importances) / 2\n",
    "# averaged_features_df = pd.DataFrame({\n",
    "#     'Feature': X.columns,\n",
    "#     'Importance': averaged_importances\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "# print(\"\\n--- Averaged Ensemble Feature Importances ---\")\n",
    "# print(averaged_features_df)\n",
    "\n",
    "# Optional: Visualization of Feature Importances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=xgb_features_df.head(10))\n",
    "plt.title('Top 10 XGBoost Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=rf_features_df.head(10))\n",
    "plt.title('Top 10 Random Forest Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaS8t1KsbfSW"
   },
   "source": [
    "5. Feature Importance Analysis: After training your final model, extract and visualize feature importances (e.g., final_xgb_model.feature_importances_ or final_rf_model.feature_importances_) to understand which input features are most influential in predicting YouTube views. This is crucial for your discussion section.\n",
    "\n",
    "6. Error Analysis (on original scale): Delve into specific cases where your best model makes large errors. Remember to use np.expm1() on your model's predictions to convert them back to the original view scale before calculating MAE on the original scale or analyzing errors. This will give you insights into where the model struggles in terms of actual view counts.\n",
    "\n",
    "7. Report Writing: Compile your findings into your research paper. Discuss your methodology, the impact of the log transformation, the results from initial cross-validation, the tuning process and its outcome, feature importances, and any insights from error analysis.\n",
    "\n",
    "By iteratively exploring these aspects, you'll gain a deeper understanding of your data and models, leading to a stronger research project."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
